import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, DataCollatorForLanguageModeling, BitsAndBytesConfig, AutoModelForSequenceClassification, CodeLlamaTokenizer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
from datetime import datetime
import bitsandbytes as bnb
import time
import json
from sklearn.metrics import classification_report
import os
import yaml
import argparse
from utils import CWE_CLASSES, LABEL_TO_CWE, CWE_TO_LABEL, get_unique_path
# import os
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

device = "cuda" if torch.cuda.is_available() else "cpu"


def load_config():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="Path to config.yaml")
    args = parser.parse_args()
    with open(args.config, "r") as f:
        config = yaml.safe_load(f)
    return config

def load_model(num_labels , model_name="codellama/CodeLlama-7b-Instruct-hf"):
    """
    è¼‰å…¥ Code LLaMA ä¸¦é€²è¡Œ 4-bit é‡åŒ–ï¼Œè¨­å®š QLoRAã€‚
    
    :param model_name: Hugging Face ä¸Šçš„æ¨¡å‹åç¨±
    :param num_labels: æ¼æ´é¡å‹æ•¸é‡
    :return: é‡åŒ–å¾Œçš„ QLoRA æ¨¡å‹ & tokenizer
    """

    # âœ… ä½¿ç”¨ bitsandbytes ä¾†é€²è¡Œ 4-bit é‡åŒ–ï¼Œç¢ºä¿ QLoRA è¨­å®šæ­£ç¢º
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,  # é€²ä¸€æ­¥å£“ç¸®æ¬Šé‡ï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
        bnb_4bit_quant_type="nf4",  # Normal Float 4ï¼Œé©åˆ QLoRA è¨“ç·´
        bnb_4bit_compute_dtype=torch.bfloat16  # é¿å… FP16 è¨ˆç®—æ™‚å‡ºç¾ NaN
    )


    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)

    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels,
        quantization_config=quantization_config,
        # device_map="auto"
        device_map={"": 0}
    )

    # model = prepare_model_for_kbit_training(model)

    return model, tokenizer


def load_data(train_file, test_file, eval_file):
    """
    è¼‰å…¥ JSON æ ¼å¼çš„ function è³‡æ–™ï¼Œä¸¦è½‰æ›ç‚º Hugging Face dataset æ ¼å¼ã€‚

    :param train_file: è¨“ç·´é›† JSON æª”æ¡ˆ
    :param test_file: æ¸¬è©¦é›† JSON æª”æ¡ˆ
    :return: è¨“ç·´é›† & æ¸¬è©¦é›†
    """

    def preprocess_json(file_path):
        with open(file_path, "r") as f:
            data = json.load(f)

        for item in data:
            if isinstance(item["buggy_lines"], dict):
                item["buggy_lines"] = "\n".join([f"Buggy Line {line}: {code}" for line, code in item["buggy_lines"].items()])
        
        # æŠŠè™•ç†å¾Œçš„ JSON å­˜å›åŸæª”æ¡ˆ
        with open(file_path, "w") as f:
            json.dump(data, f, indent=4)

    # é è™•ç†ä¸¦è¼‰å…¥è³‡æ–™
    preprocess_json(train_file)
    preprocess_json(test_file)
    preprocess_json(eval_file)
    dataset = load_dataset("json", data_files={"train": train_file, "eval": eval_file, "test": test_file})
    return dataset["train"],  dataset["eval"], dataset["test"]



def setup_tokenizer(model, tokenizer):
    """
    è¨­å®š tokenizer ä»¥é©æ‡‰ function ç´šåˆ¥çš„è¨“ç·´è³‡æ–™ã€‚
    """
    tokenizer.pad_token = tokenizer.eos_token  # ğŸ”¹ æŠŠ `pad_token` è¨­å®šæˆ `eos_token`
    tokenizer.pad_token_id = tokenizer.eos_token_id  # ğŸ”¹ ç¢ºä¿ `pad_token_id` è¨­å®šæ­£ç¢º
    tokenizer.padding_side = "left"  # Code Llama é è¨­å·¦å´ `padding`
    model.config.pad_token_id = tokenizer.pad_token_id

    return tokenizer

# def build_conversation(system_prompt, turns):
#     """
#     <s>
#         [INST]
#             <<SYS>>{{ system_prompt }}<</SYS>>
#         {{ user_msg_1 }}
#         [/INST]
#         {{ model_amswer_1 }}
#     </s>
#     <s>
#         [INST]
#         {{ user_msg_2 }}
#         [/INST]
#         {{ model_amswer_2 }}
#     </s>
#     """
#     conv = f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n"
#     for i, (user_msg, model_reply) in enumerate(turns):
#         conv += f"{user_msg} [/INST] {model_reply}</s>"
#         if i < len(turns) - 1:
#             conv += "<s>[INST] "
#     return conv

def build_prompt(code, buggy_lines):
    """
    <s>
        [INST]
            <<SYS>>{{ system_prompt }}<</SYS>>
        {{ user_msg_1 }}
        [/INST]
        {{ model_amswer_1 }}
    </s>
    <s>
        [INST]
        {{ user_msg_2 }}
        [/INST]
        {{ model_amswer_2 }}
    </s>
    """
    
    system_prompt = f"""You are a cybersecurity expert. 
                        Based on the given code and buggy line, Answer with only one CWE-ID:\n.
                        Do not explain or provide details."""
    user_msg = f""" [function_code]:\n{code}\n
                    [known buggy line]:\n{buggy_lines}\n
                    [Response]:"""
    return f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n{user_msg} [/INST]</s>" 

def tokenize(tokenizer,sample):
    """
    - å°‡ function è½‰æ›ç‚º Tokenï¼Œä¸¦å°‡ function æœ¬èº«ç•¶ä½œ labels é€²è¡Œè‡ªç›£ç£å­¸ç¿’ã€‚
    """

    prompt = build_prompt(sample["code"], sample["buggy_lines"])
    tokenized = tokenizer(prompt, truncation=True, max_length=1024, padding="max_length")

    # input_ids = tokenized["input_ids"].squeeze(0)
    # attention_mask = tokenized["attention_mask"].squeeze(0)
    
    # å°‡ cwe_id è½‰ç‚ºæ•´æ•¸æ¨™ç±¤
    # labels = torch.tensor(CWE_TO_LABEL[sample["cwe_id"]], dtype=torch.long)
    tokenized["labels"] = CWE_TO_LABEL[sample["cwe_id"]]
    # labels = input_ids.clone() # Causal LM è¦æ±‚ labels èˆ‡ input_ids ä¸€æ¨£
    # return {
    #     "input_ids": input_ids, 
    #     "attention_mask": attention_mask,
    #     "labels": labels,
    # }
    return tokenized

def setup_qlora(model):
    """
    è¨­å®š LoRA å¾®èª¿é…ç½®ï¼Œæ‡‰ç”¨è‡³æ¨¡å‹ã€‚

    :param model: é‡åŒ–å¾Œçš„ Code LLaMA æ¨¡å‹
    :return: å¥—ç”¨ QLoRA è¨­å®šå¾Œçš„æ¨¡å‹
    """
    # é—œé–‰æ¬Šé‡çš„ requires_gradï¼ˆä¸æ›´æ–°åŸå§‹æ¬Šé‡ï¼Œåªæ›´æ–° LoRA å±¤ï¼‰
    model = prepare_model_for_kbit_training(model)

    lora_config = LoraConfig(
        r=8, # Rank
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="SEQ_CLS", # é æ¸¬ function çš„æ¼æ´é¡å‹
        # task_type="CAUSAL_LM", # ç”Ÿæˆ function è£œå…¨æˆ–ä¿®å¾©æ¼æ´
    )

    model = get_peft_model(model, lora_config)
    
    # åˆ—å‡ºç›®å‰å“ªäº›åƒæ•¸æ˜¯å¯è¨“ç·´çš„ï¼Œä¸¦è¨ˆç®— å¯è¨“ç·´åƒæ•¸çš„æ¯”ä¾‹
    model.print_trainable_parameters()
    return model

def train_model(model, tokenizer, train_data, test_data, output_dir, config):
    """
    è¨“ç·´ QLoRA å¾®èª¿çš„ Code LLaMAï¼Œé€²è¡Œ function ç´šåˆ¥æ¼æ´åˆ†é¡ã€‚

    :param model: é‡åŒ–ä¸¦åŠ ä¸Š QLoRA çš„æ¨¡å‹
    :param tokenizer: Tokenizer
    :param train_data: è¨“ç·´é›†
    :param test_data: æ¸¬è©¦é›†
    :param output_dir: è¨“ç·´çµæœå„²å­˜è·¯å¾‘
    """
    model.to(device)

    train_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=config["per_device_train_batch_size"], # å–®å€‹ GPU è¨“ç·´çš„ batch size
        per_device_eval_batch_size=config["per_device_eval_batch_size"],
        gradient_accumulation_steps=config["gradient_accumulation_steps"], # æ¢¯åº¦ç´¯ç© 32 æ¬¡ï¼Œç›¸ç•¶æ–¼ batch size = 2 * 32 = 64
        warmup_steps=config["warmup_steps"], # è¨“ç·´é–‹å§‹å‰çš„å­¸ç¿’ç‡ Warmup æ­¥æ•¸ï¼Œé˜²æ­¢å­¸ç¿’ç‡éå¤§å°è‡´ç™¼æ•£
        eval_strategy=config["eval_strategy"], # æ¯ N å€‹ step é€²è¡Œä¸€æ¬¡é©—è­‰
        save_strategy=config["save_strategy"], # æ¯ N å€‹ step å„²å­˜ä¸€æ¬¡ checkpoint
        eval_steps=config["eval_steps"], # æ¯ 50 å€‹ step é€²è¡Œä¸€æ¬¡é©—è­‰
        save_steps=config["save_steps"], # æ¯ 100 å€‹ step å„²å­˜æ¨¡å‹
        logging_steps=config["logging_steps"], # æ¯ 10 å€‹ step é¡¯ç¤ºä¸€æ¬¡ log
        learning_rate=float(config["learning_rate"]), # è¨­å®šå­¸ç¿’ç‡ï¼ˆæ¯”è¼ƒé©åˆ LoRA è¨“ç·´ï¼‰
        num_train_epochs=config["num_train_epochs"], # è¨“ç·´ 3 å€‹ epoch
        weight_decay=config["weight_decay"], # L2 æ­£å‰‡åŒ–ï¼Œé˜²æ­¢éæ“¬åˆ
        bf16=config["bf16"], # ä½¿ç”¨ `bfloat16`ï¼ˆé©åˆ RTX 40 ç³»åˆ— GPUï¼‰
        optim=config["optim"], # AdamW 8-bit é‡åŒ–ï¼Œé™ä½è¨˜æ†¶é«”éœ€æ±‚
        logging_dir=config["logging_dir"], # è¨“ç·´éç¨‹ Log å„²å­˜è·¯å¾‘
        run_name=f"classification-codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}", # è¨­å®šè¨“ç·´åç¨±ï¼ŒåŒ…å«æ™‚é–“æˆ³è¨˜
    )

    trainer = Trainer(
        model=model, # ä½ çš„ LoRA è¨“ç·´æ¨¡å‹
        args=train_args, # è¨“ç·´åƒæ•¸ï¼ˆTrainingArgumentsï¼‰
        train_dataset=train_data, # è¨“ç·´é›†
        eval_dataset=test_data, # æ¸¬è©¦é›†
        tokenizer=tokenizer, # Tokenizer
        # data_collator=DataCollatorForLanguageModeling(tokenizer,mlm=False)
        data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
    )

    # é–‹å§‹è¨“ç·´
    model.config.use_cache = False  # QLoRA è¨“ç·´æ™‚è¦é—œé–‰ cache
    trainer.train()  # é–‹å§‹è¨“ç·´
    model.config.use_cache = True  # è¨“ç·´å®Œæˆå¾Œé‡æ–°é–‹å•Ÿ
    # trainer.train(resume_from_checkpoint="/home/tu/exp/Training/Code_llama/AZOO/result/finetune/QLoRA_Codellama_classification_7b_3/checkpoint-500") # resume_from_checkpoint=True æœƒå¾ä¸Šæ¬¡å„²å­˜çš„ checkpoint é–‹å§‹è¨“ç·´
    

def main():

    config = load_config()

    model_name = config["model_name"]
    
    if config["task"] == "multi":
        num_labels = len(CWE_CLASSES)
    elif config["task"] == "binary":
        num_labels = 2
    else:
        raise ValueError(f"Unsupported task type: {config['task']}. Use 'multi' or 'binary'.")


    print("ğŸš€ è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer ...")
    model, tokenizer = load_model(num_labels, model_name)

    print("ğŸ“‚ è¼‰å…¥æ•¸æ“šé›† ...")
    train_data, eval_data, test_data = load_data(
        train_file = config["train_file"],
        eval_file= config["eval_file"],
        test_file= config["test_file"]
    )
    print("âš™ï¸ è¨­å®š tokenizer ...")
    tokenizer = setup_tokenizer(model, tokenizer)

    print("ğŸ”„ Tokenizing è¨“ç·´/æ¸¬è©¦æ•¸æ“š ...")
    tokenized_train_data = train_data.map(lambda sample: tokenize(tokenizer, sample), remove_columns=train_data.column_names)
    tokenized_eval_data = eval_data.map(lambda sample: tokenize(tokenizer, sample), remove_columns=eval_data.column_names)
    tokenized_test_data = test_data.map(lambda sample: tokenize(tokenizer, sample), remove_columns=test_data.column_names)
    # tokenized_train_data = train_data.map(
    #     lambda x: {
    #         k: torch.tensor(v, dtype=torch.long) if isinstance(v, list) else v
    #         for k, v in tokenize(tokenizer, x).items()
    #     },
    #     remove_columns=train_data.column_names
    # )

    # tokenized_test_data = test_data.map(
    #     lambda x: {
    #         k: torch.tensor(v, dtype=torch.long) if isinstance(v, list) else v
    #         for k, v in tokenize(tokenizer, x).items()
    #     },
    #     remove_columns=test_data.column_names
    # )

    # tokenized_eval_data = eval_data.map(
    #     lambda x: {
    #         k: torch.tensor(v, dtype=torch.long) if isinstance(v, list) else v
    #         for k, v in tokenize(tokenizer, x).items()
    #     },
    #     remove_columns=eval_data.column_names
    # )
   
    print("ğŸ›ï¸ è¨­å®š QLoRA æ¨¡å‹ ...")
    model = setup_qlora(model)

    # output_file = get_unique_path(config["output_dir"])
    output_file = "/home/tu/exp/Training/Code_llama/AZOO/result/finetune/QLoRA_Codellama_classification_7b_4"
    

    print("ğŸ¯ é–‹å§‹è¨“ç·´ ...")
    start_time = time.time()
    train_model(model, tokenizer, tokenized_train_data, tokenized_eval_data, output_file, config)
    end_time = time.time()
    total_time = end_time - start_time    

    # å„²å­˜è¨“ç·´æ™‚é–“
    training_time_path = os.path.join(output_file, "config.txt")
    with open(training_time_path, "w") as f:
        f.write(f"Config: {config}\n")
        f.write(f"Training time: {round(total_time, 2)} seconds\n")
    print(f"â±ï¸ Training time saved to: {training_time_path}")

if __name__ == "__main__":
    main()
    