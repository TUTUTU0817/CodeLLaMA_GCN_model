import random
import numpy as np
import torch
import json
from networkx.drawing.nx_pydot import read_dot
from torch_geometric.data import Data
import os
import pickle
from sklearn.metrics import classification_report
from datasets import load_dataset
from transformers import  AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig
from peft import PeftModel

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

CWE_CLASSES = [
    # "normal",
    "CWE-89",   # SQL Injection
    "CWE-200",  # Information Exposure
    "CWE-250",  # Execution with Unnecessary Privileges
    "CWE-276",  # Incorrect Default Permissions
    "CWE-295",  # Improper Certificate Validation
    "CWE-312",  # Cleartext Storage of Sensitive Information
    "CWE-327",  # Use of a Broken or Risky Cryptographic Algorithm
    "CWE-649",  # Reliance on Obfuscation or Encryption Without Integrity
    "CWE-749",  # Exposed Dangerous Method or Function
]

LABEL_TO_CWE = {i: cwe for i, cwe in enumerate(CWE_CLASSES)}
CWE_TO_LABEL = {cwe: i for i, cwe in enumerate(CWE_CLASSES)}


node_type_dict = {
    'ANNOTATION_LITERAL': 0, 
    'ANNOTATION_PARAMETER': 1, 
    'ANNOTATION_PARAMETER_ASSIGN': 2, 
    'ARRAY_INITIALIZER': 3, 
    'BLOCK': 4, 
    'CONTROL_STRUCTURE': 5,
    'FIELD_IDENTIFIER': 6, 
    'IDENTIFIER': 7,
    'JUMP_TARGET': 8,
    'LITERAL': 9, 
    'LOCAL': 10,  
    'METHOD': 11, 
    'METHOD_RETURN': 12, 
    'MODIFIER': 13,
    'PARAM': 14,
    'RETURN': 15,
    'TYPE_REF': 16,
    'UNKNOWN': 17,
}

edge_type_dict = {
    "AST": 0,
    "CFG": 1,
    "DDG": 2,
    "CDG": 3,
}

# # è®€å–è³‡æ–™
# def load_json_dataset(json_file):
#     """ è®€å– JSON è¨“ç·´é›† """
#     with open(json_file, 'r') as f:
#         dataset = json.load(f)
#     return dataset

# # åœ–çš„å‰è™•ç†
# def get_code_embedding(llm_model, tokenizer, code_snippet):
#     """ å–å¾— LLM çš„ code embeddingï¼ˆmean poolingï¼‰"""
#     tokens = tokenizer(code_snippet, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
#     with torch.no_grad():
#         embedding = llm_model.base_model(**tokens).last_hidden_state.mean(dim=1)
#     return embedding.squeeze(0)


# def create_pyg_graph_from_dot(llm_model, tokenizer, dot_file):
#     """ è®€å– DOT åœ–ä¸¦è½‰æ›ç‚º PyG åœ– """
#     nx_graph = read_dot(dot_file)
#     node_map = {node: i for i, node in enumerate(nx_graph.nodes())}
#     edge_index = torch.tensor([
#         [node_map[src], node_map[dst]] for src, dst in nx_graph.edges()
#     ]).T

#     node_features = []
#     for node in nx_graph.nodes():
#         code = nx_graph.nodes[node].get("code", "")
#         emb = get_code_embedding(llm_model, tokenizer, code).to("cpu")
#         node_features.append(emb)
#     x = torch.stack(node_features)
#     return Data(x=x, edge_index=edge_index)

# def get_pyg_graph(model, tokenizer, dot_file, cache_dir="/home/tu/exp/Training/GNN/cache"):
#     """ 
#     è®€å– JSON è¨“ç·´é›†ï¼Œå¾ DOT æª”æ¡ˆå‰µå»º PyG åœ–ï¼Œä¸¦è¿”å›è³‡æ–™é›†ã€‚
    
#     åƒæ•¸:
#     - model: ç”¨ä¾†ç”Ÿæˆç¨‹å¼ç¢¼åµŒå…¥çš„æ¨¡å‹ï¼ˆCode LLaMAï¼‰ã€‚
#     - tokenizer: ç”¨ä¾†è™•ç†ç¨‹å¼ç¢¼çš„åˆ†è©å™¨ï¼ˆCode LLaMAï¼‰ã€‚
#     - dot_file: åŒ…å«åœ–çµæ§‹çš„ dot æ–‡ä»¶ã€‚
#     - label_map: CWE é¡åˆ¥çš„å°æ‡‰æ˜ å°„ã€‚
#     - cache_dir: å¿«å–è³‡æ–™å¤¾ï¼Œç”¨ä¾†å„²å­˜å·²è™•ç†çš„åœ–ã€‚
    
#     è¿”å›:
#     - pyg_graph: åœ–ã€‚
#     """
#     os.makedirs(cache_dir, exist_ok=True)  # ç¢ºä¿å¿«å–è³‡æ–™å¤¾å­˜åœ¨

#     cache_file = os.path.join(cache_dir, os.path.basename(dot_file) + ".pkl")
    
#     # å¦‚æœæœ‰å¿«å–ï¼Œå°±ç›´æ¥è®€å–
#     if os.path.exists(cache_file):
#         with open(cache_file, "rb") as f:
#             pyg_graph = pickle.load(f) 
#     else:
#         pyg_graph = create_pyg_graph_from_dot(model, tokenizer, dot_file)
#         with open(cache_file, "wb") as f:
#             pickle.dump(pyg_graph, f)
#     return pyg_graph


def load_model(num_labels, model_name="codellama/CodeLlama-7b-Instruct-hf"):
    """
    è¼‰å…¥ Code LLaMA ä¸¦é€²è¡Œ 4-bit é‡åŒ–ï¼Œè¨­å®š QLoRAã€‚
    
    :param model_name: Hugging Face ä¸Šçš„æ¨¡å‹åç¨±
    :param num_labels: æ¼æ´é¡å‹æ•¸é‡
    :return: é‡åŒ–å¾Œçš„ QLoRA æ¨¡å‹ & tokenizer
    """

    # âœ… ä½¿ç”¨ bitsandbytes ä¾†é€²è¡Œ 4-bit é‡åŒ–ï¼Œç¢ºä¿ QLoRA è¨­å®šæ­£ç¢º
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,  # é€²ä¸€æ­¥å£“ç¸®æ¬Šé‡ï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
        bnb_4bit_quant_type="nf4",  # Normal Float 4ï¼Œé©åˆ QLoRA è¨“ç·´
        bnb_4bit_compute_dtype=torch.bfloat16  # é¿å… FP16 è¨ˆç®—æ™‚å‡ºç¾ NaN
    )


    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)

    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels,
        quantization_config=quantization_config,
        # device_map="auto"
        device_map={"": 0}
    )

    # model = prepare_model_for_kbit_training(model)

    return model, tokenizer

def load_fine_tuned_model(num_labels, model_name, checkpoint_path):
    """
    è¼‰å…¥å¾®èª¿å¾Œçš„æ¨¡å‹
    :param model_name: æ¨¡å‹åç¨±
    :return: è¼‰å…¥çš„æ¨¡å‹èˆ‡ tokenizer
    """

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,  # âœ… 4-bit é‡åŒ–ï¼ˆè‹¥ç”¨ 8-bitï¼Œæ”¹ç‚º load_in_8bit=Trueï¼‰
        bnb_4bit_compute_dtype=torch.float16,  
        bnb_4bit_use_double_quant=True,  
        bnb_4bit_quant_type="nf4"  
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    base_model = AutoModelForSequenceClassification.from_pretrained(
        model_name, 
        num_labels=num_labels,
        quantization_config=quantization_config,
        device_map={"": 0}  
    )
    model = PeftModel.from_pretrained(base_model, checkpoint_path)

    # print(model.peft_config)
    return model, tokenizer


def load_data(train_file, test_file, eval_file):
    """
    è¼‰å…¥ JSON æ ¼å¼çš„ function è³‡æ–™ï¼Œä¸¦è½‰æ›ç‚º Hugging Face dataset æ ¼å¼ã€‚

    :param train_file: è¨“ç·´é›† JSON æª”æ¡ˆ
    :param test_file: æ¸¬è©¦é›† JSON æª”æ¡ˆ
    :return: è¨“ç·´é›† & æ¸¬è©¦é›†
    """

    def preprocess_json(file_path):
        with open(file_path, "r") as f:
            data = json.load(f)

        for item in data:
            if isinstance(item["buggy_lines"], dict):
                item["buggy_lines"] = "\n".join([f"Buggy Line {line}: {code}" for line, code in item["buggy_lines"].items()])
        
        # æŠŠè™•ç†å¾Œçš„ JSON å­˜å›åŸæª”æ¡ˆ
        with open(file_path, "w") as f:
            json.dump(data, f, indent=4)

    # é è™•ç†ä¸¦è¼‰å…¥è³‡æ–™
    preprocess_json(train_file)
    preprocess_json(test_file)
    preprocess_json(eval_file)
    dataset = load_dataset("json", data_files={"train": train_file, "eval": eval_file, "test": test_file})
    return dataset["train"],  dataset["eval"], dataset["test"]


def setup_tokenizer(model, tokenizer):
    """
    è¨­å®š tokenizer ä»¥é©æ‡‰ function ç´šåˆ¥çš„è¨“ç·´è³‡æ–™ã€‚
    """
    tokenizer.pad_token = tokenizer.eos_token  # ğŸ”¹ æŠŠ `pad_token` è¨­å®šæˆ `eos_token`
    tokenizer.pad_token_id = tokenizer.eos_token_id  # ğŸ”¹ ç¢ºä¿ `pad_token_id` è¨­å®šæ­£ç¢º
    tokenizer.padding_side = "left"  # Code Llama é è¨­å·¦å´ `padding`
    model.config.pad_token_id = tokenizer.pad_token_id

    return tokenizer



def build_prompt(code, buggy_lines):
    """
    <s>
        [INST]
            <<SYS>>{{ system_prompt }}<</SYS>>
        {{ user_msg_1 }}
        [/INST]
        {{ model_amswer_1 }}
    </s>
    <s>
        [INST]
        {{ user_msg_2 }}
        [/INST]
        {{ model_amswer_2 }}
    </s>
    """
    
    system_prompt = f"""You are a cybersecurity expert. 
                        Based on the given code and buggy line, Answer with only one CWE-ID:\n.
                        Do not explain or provide details."""
    user_msg = f""" [function_code]:\n{code}\n
                    [known buggy line]:\n{buggy_lines}\n
                    [Response]:"""
    return f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n{user_msg} [/INST]</s>" 

def build_prompt_B(code):
    """
    <s>
        [INST]
            <<SYS>>{{ system_prompt }}<</SYS>>
        {{ user_msg_1 }}
        [/INST]
        {{ model_amswer_1 }}
    </s>
    <s>
        [INST]
        {{ user_msg_2 }}
        [/INST]
        {{ model_amswer_2 }}
    </s>
    """
    
    system_prompt = f"""You are a cybersecurity expert. 
                        Based on the given code and buggy line, Answer with only one CWE-ID:\n.
                        Do not explain or provide details."""
    user_msg = f""" [function_code]:\n{code}\n
                    [Response]:"""
    return f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n{user_msg} [/INST]</s>" 

# def tokenize(tokenizer, sample):
#     system_prompt = "You are a cybersecurity expert. Identify the CWE-ID of the following buggy code.Answer with only one CWE-ID (e.g., CWE-200). Do not explain or provide details."
#     user_msg = f"[function_code]:\n{sample['code']}\n[known buggy line]:\n{sample['buggy_line']}\n[Response]:"
#     target = sample["cwe_id"]
#     full_prompt = build_prompt(system_prompt, user_msg, target)

#     tokenized = tokenizer(full_prompt, truncation=True, padding="max_length", max_length=1024, return_tensors="pt")
#     return {
#         "input_ids": tokenized["input_ids"].squeeze(0),
#         "attention_mask": tokenized["attention_mask"].squeeze(0)
#     }

# def tokenize(tokenizer,sample):
#     """
#     - å°‡ function è½‰æ›ç‚º Tokenï¼Œä¸¦å°‡ function æœ¬èº«ç•¶ä½œ labels é€²è¡Œè‡ªç›£ç£å­¸ç¿’ã€‚
#     """

#     prompt = build_prompt(sample["code"], sample["buggy_lines"])
#     tokenized = tokenizer(prompt, truncation=True, max_length=1024, padding="max_length")

#     # input_ids = tokenized["input_ids"].squeeze(0)
#     # attention_mask = tokenized["attention_mask"].squeeze(0)
    
#     # å°‡ cwe_id è½‰ç‚ºæ•´æ•¸æ¨™ç±¤
#     # labels = torch.tensor(CWE_TO_LABEL[sample["cwe_id"]], dtype=torch.long)
#     tokenized["labels"] = CWE_TO_LABEL[sample["cwe_id"]]
#     # labels = input_ids.clone() # Causal LM è¦æ±‚ labels èˆ‡ input_ids ä¸€æ¨£
#     # return {
#     #     "input_ids": input_ids, 
#     #     "attention_mask": attention_mask,
#     #     "labels": labels,
#     # }
#     return tokenized

def get_unique_path(base_path):
    if not os.path.exists(base_path):
        return base_path
    counter = 2
    while True:
        new_path = f"{base_path}_{counter}"
        if not os.path.exists(new_path):
            return new_path
        counter += 1
        
def get_unique_filename(output_path, base_name):
    counter = 1
    while True:
        file_name = f"{base_name}_batch_{counter}.pkl"
        file_path = os.path.join(output_path, file_name)
        if not os.path.exists(file_path):
            return file_path
        counter += 1

def save_result(results, report, output_dir, training_time, config):
    """
    å„²å­˜é æ¸¬çµæœèˆ‡åˆ†é¡å ±å‘Šåˆ°æŒ‡å®šçš„ç›®éŒ„ã€‚
    :param results: é æ¸¬çµæœ
    :param report: åˆ†é¡å ±å‘Š
    :param output_dir: å„²å­˜çš„ç›®éŒ„
    :param training_time: è¨“ç·´æ™‚é–“
    :param config: æ¨¡å‹é…ç½®
    """

    json_path = get_unique_path(os.path.join(output_dir, "best_fusion_prediction_results")) + ".json"
    with open(json_path, "w") as jf:
        json.dump(results, jf, indent=2)
    print(f"âœ… é æ¸¬çµæœå·²å„²å­˜è‡³: {json_path}")
    
    txt_path = get_unique_path(os.path.join(output_dir, "best_fusion_classification_report")) + ".txt"
    with open(txt_path, "w") as tf:
        tf.write(report)
        tf.write(f"\n\nğŸ•’ Training time: {round(training_time, 2)} seconds\n")
        tf.write(f"\n\nğŸ“ Model configuration:\n {config}")
    print(f"âœ… åˆ†é¡å ±å‘Šå·²å„²å­˜è‡³: {txt_path}")


# def save_result(results, result_output_dir, report_output_dir, training_time, config):


#     # æª¢æŸ¥ä¸¦ç”Ÿæˆå”¯ä¸€çš„æª”æ¡ˆåç¨±
#     def get_unique_file_name(file_path):
#         if os.path.exists(file_path):
#             base_name, ext = os.path.splitext(file_path)
#             counter = 2
#             while os.path.exists(f"{base_name}_{counter}{ext}"):
#                 counter += 1
#             return f"{base_name}_{counter}{ext}"
#         return file_path
#     y_true = [res["y_true"] for res in results]
#     y_pred = [res["y_pred"] for res in results]
    
#     report = classification_report(
#         y_true, 
#         y_pred, 
#         target_names=LABEL_TO_CWE.values(), 
#         digits=4,
#         labels=list(LABEL_TO_CWE.keys()),  # æŒ‡å®šè¦è¨ˆç®—çš„é¡åˆ¥
#         zero_division=0  # è‹¥æŸé¡åˆ¥æ²’å‡ºç¾ï¼Œä¹Ÿä¸å ±éŒ¯
# )

#     result_output_dir = get_unique_file_name(result_output_dir)
#     report_output_dir = get_unique_file_name(report_output_dir)

#     with open(result_output_dir, "w", encoding="utf-8") as f:
#         json.dump(results, f, indent=4, ensure_ascii=False)

#     with open(report_output_dir, "w", encoding="utf-8") as f:
#         f.write(report)  # æ¥å¯«å…¥ txt æª”æ¡ˆ
#         f.write(f"\n\nğŸ•’ Training time: {round(training_time, 2)} seconds\n")
#         f.write(f"\n\nğŸ“ Model configuration:\n {config}")
#     return report


class JsonGraphDataset(torch.utils.data.Dataset):
    def __init__(self, json_file, cwe2idx):
        with open(json_file) as f:
            entries = json.load(f)

        self.entries = [e for e in entries if e.get("graph") and os.path.exists(e["graph"])]
        self.cwe2idx = cwe2idx

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        entry = self.entries[idx]
        data = torch.load(entry["graph"], map_location='cpu')
        
        cwe_id = entry["cwe_id"]
        label_idx = self.cwe2idx.get(cwe_id, -1)  # é è¨­ -1 ä»£è¡¨æœªçŸ¥
        assert label_idx >= 0, f"Unknown CWE-ID: {cwe_id}"

        data.y = torch.tensor(label_idx, dtype=torch.long)
        data.file_name = entry["fullName"]  # optional
        return data
    
def save_result(results, report, output_dir, model_name):

    json_path = os.path.join(output_dir, f"{model_name}_results.json")
    with open(json_path, "w") as jf:
        json.dump(results, jf, indent=2)
    print(f"âœ… é æ¸¬çµæœå·²å„²å­˜è‡³: {json_path}")
    
    txt_path = os.path.join(output_dir, f"{model_name}_classification_report.txt")
    with open(txt_path, "w") as tf:
        tf.write(report)
    print(f"âœ… åˆ†é¡å ±å‘Šå·²å„²å­˜è‡³: {txt_path}")


def set_seed(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # å¤š GPU æ™‚ä¹Ÿå›ºå®š
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def stratified_split(data, labels, random_statem, test_size=0.1):
    """
    æ ¹æ“šæ¨™ç±¤é€²è¡Œåˆ†å±¤æŠ½æ¨£ï¼Œå°‡è³‡æ–™åˆ†ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†ã€‚
    """
    from sklearn.model_selection import train_test_split

    # ä½¿ç”¨ stratify åƒæ•¸é€²è¡Œåˆ†å±¤æŠ½æ¨£
    train_data, test_data, train_labels, test_labels = train_test_split(
        data, labels, test_size=test_size, random_state=random_statem, stratify=labels
    )
    # å†å¾å‰©é¤˜çš„è¨“ç·´é›†ä¸­åˆ‡å‡ºé©—è­‰é›†
    train_data, eval_data, train_labels, eval_labels = train_test_split(
        train_data, train_labels, test_size=test_size / (1 - test_size), random_state=random_statem, stratify=train_labels)
    
    return  train_data, train_labels, eval_data, eval_labels, test_data, test_labels


