import gc
import json
import os
import random
import time
import pandas as pd
import re
import torch
import networkx as nx
from torch_geometric.data import Data
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch_geometric.utils import to_networkx
import pickle
import logging
import psutil

# ==== è·¯å¾‘è¨­å®šå€ ====
BASE_DIR = "/home/tu/exp/EXP_final/Dataset/multi"
# TRAIN_PT = os.path.join(BASE_DIR, "vul_train.pt")
# EVAL_PT = os.path.join(BASE_DIR, "vul_eval.pt")
# TEST_PT = os.path.join(BASE_DIR, "vul_test.pt")
# TRAIN_OUT = os.path.join(BASE_DIR, "vul_train.pt")
# EVAL_OUT = os.path.join(BASE_DIR, "vul_eval.pt")
# TEST_OUT = os.path.join(BASE_DIR, "vul_test.pt")
EXAMPLE_PT = "/home/tu/exp/EXP_final/Example_output/result_with_fullgraph.pt"
EXAMPLE_OUT_PT = "/home/tu/exp/EXP_final/Example_output/result_with_fullgraph.pt"
TMP_DIR = "/home/tu/exp/EXP_final/preprocessing/graph_processing"
LOG_PATH = "/home/tu/exp/EXP_final/Example_output/output_graph_process_sliced.log"
SLICE_MODE = "buggy_lines"  # å¯é¸ "buggy_lines" æˆ– "node_type"
# ==== è¨­å®š logging åŸºæœ¬é…ç½® ====
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler(LOG_PATH, mode="a"),
                              logging.StreamHandler()])
# ==== è¼”åŠ©å‡½ç¤º ====
def log_memory_usage():
    """è¨˜éŒ„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    mem_mb = mem_info.rss / 1024 / 1024
    logging.info(f"ğŸ§  è¨˜æ†¶é«”ä½¿ç”¨: {mem_mb:.2f} MB")
    return mem_mb

def _build_sliced_data_optimized(data, slice_node_ids):
    """å„ªåŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨ tensor æ“ä½œï¼Œé¿å… NetworkX"""
    if not slice_node_ids:
        logging.warning("âš ï¸ åˆ‡å‰²å¾Œç¯€é»ç‚ºç©ºï¼Œè¿”å›ç©ºåœ–")
        return Data(
            x=torch.empty((0, data.x.size(1))),
            edge_index=torch.empty((2, 0), dtype=torch.long),
            edge_attr=torch.empty((0, data.edge_attr.size(1))) if hasattr(data, 'edge_attr') and data.edge_attr is not None else None,
            types=[], codes=[], edge_type=[], line=[]
        )
    
    # è½‰æ›ç‚ºåˆ—è¡¨ä¸¦æ’åº
    new_node_ids = sorted(list(slice_node_ids))
    node_id_map = {old_id: new_id for new_id, old_id in enumerate(new_node_ids)}
    
    # ä½¿ç”¨ tensor æ“ä½œç¯©é¸é‚Š
    edge_index = data.edge_index
    edge_mask = torch.zeros(edge_index.size(1), dtype=torch.bool)
    
    for i in range(edge_index.size(1)):
        src, dst = edge_index[0, i].item(), edge_index[1, i].item()
        if src in slice_node_ids and dst in slice_node_ids:
            edge_mask[i] = True
    
    # ç¯©é¸æœ‰æ•ˆé‚Š
    valid_edges = edge_index[:, edge_mask]
    
    # é‡æ–°æ˜ å°„ç¯€é» ID
    new_edge_index = torch.zeros_like(valid_edges)
    for i in range(valid_edges.size(1)):
        src, dst = valid_edges[0, i].item(), valid_edges[1, i].item()
        new_edge_index[0, i] = node_id_map[src]
        new_edge_index[1, i] = node_id_map[dst]
    
    # ç¯©é¸é‚Šçš„å±¬æ€§
    new_edge_type = [data.edge_type[i] for i, mask in enumerate(edge_mask) if mask] if hasattr(data, 'edge_type') else []
    new_edge_attr = data.edge_attr[edge_mask] if hasattr(data, 'edge_attr') and data.edge_attr is not None else None
    
    logging.info(f"âš™ï¸ åˆ‡å‰²å¾Œé‚Šæ•¸é‡: {new_edge_index.size(1)}")
    
    # é‡å»º Data
    sliced_data = Data(
        x=data.x[new_node_ids],
        edge_index=new_edge_index,
        edge_attr=new_edge_attr,
        y=data.y if hasattr(data, 'y') else None,
        types=[data.types[i] for i in new_node_ids],
        codes=[data.codes[i] for i in new_node_ids],
        edge_type=new_edge_type,
        line=[data.line[i] for i in new_node_ids]
    )
    
    return sliced_data

def slice_graph(data, mode="buggy", target_lines=None):
    """
    é€šç”¨åœ–åˆ‡å‰²å‡½å¼ã€‚
    
    Parameters:
        data: PyG Graph ç‰©ä»¶
        mode: "buggy" è¡¨ç¤ºç”¨æ¼æ´è¡Œè£åˆ‡, "type" è¡¨ç¤ºç”¨ CALL/IDENTIFIER ç¯€é»è£åˆ‡
        target_lines: åƒ…åœ¨ mode="buggy" æ™‚éœ€è¦çš„åƒæ•¸ (list of int)
    Returns:
        è£åˆ‡å¾Œçš„ PyG Graph
    """
    try:
        # === 1. æ‰¾å‡ºç›®æ¨™ç¯€é» ===
        if mode == "buggy_lines":
            if not target_lines:
                logging.warning("âš ï¸ Buggy æ¨¡å¼éœ€æä¾› target_linesï¼Œè¿”å›åŸåœ–")
                return data
            target_lines_set = set(map(str, target_lines))
            target_node_ids = [i for i, ln in enumerate(data.line) if str(ln) in target_lines_set]
            if not target_node_ids:
                logging.warning("âš ï¸ æ‰¾ä¸åˆ°å°æ‡‰æ¼æ´è¡Œç¯€é»ï¼Œè¿”å›åŸåœ–")
                return data
        elif mode == "node_type":
            target_node_ids = [i for i, t in enumerate(data.types) if t in {"CALL", "IDENTIFIER"}]
            if not target_node_ids:
                logging.warning("âš ï¸ æ‰¾ä¸åˆ° CALL/IDENTIFIER ç¯€é»ï¼Œè¿”å›åŸåœ–")
                return data
        else:
            logging.error(f"âŒ ä¸æ”¯æ´çš„è£åˆ‡æ¨¡å¼: {mode}")
            return data

        # === 2. å»ºç«‹é„°æ¥é—œä¿‚ ===
        edge_index = data.edge_index
        predecessors, successors, edge_types = {}, {}, {}

        for i in range(edge_index.size(1)):
            src, dst = edge_index[0, i].item(), edge_index[1, i].item()
            edge_type = data.edge_type[i] if i < len(data.edge_type) else 'UNKNOWN'
            predecessors.setdefault(dst, []).append(src)
            successors.setdefault(src, []).append(dst)
            edge_types[(src, dst)] = edge_type

        # === 3. éæ­·: æ‰¾å‡ºèˆ‡ç›®æ¨™ç¯€é»ç›¸é€£çš„é‚Š (DDG, CDG) ===
        slice_node_ids = set()
        for node_id in target_node_ids:
            stack = [node_id]
            visited = set()
            while stack:
                current = stack.pop()
                if current in visited:
                    continue
                visited.add(current)
                slice_node_ids.add(current)
                neighbors = predecessors.get(current, []) + successors.get(current, [])
                for neighbor in neighbors:
                    if neighbor in visited:
                        continue
                    edge_type = edge_types.get((current, neighbor)) or edge_types.get((neighbor, current))
                    if edge_type in {"DDG", "CDG"}:
                        stack.append(neighbor)

        # === 4. åŠ å…¥ AST/CFG é„°å±… ===
        additional_nodes = set()
        for node_id in slice_node_ids:
            neighbors = predecessors.get(node_id, []) + successors.get(node_id, [])
            for neighbor in neighbors:
                if neighbor in slice_node_ids:
                    continue
                edge_type = edge_types.get((node_id, neighbor)) or edge_types.get((neighbor, node_id))
                if edge_type in {"AST", "CFG"}:
                    additional_nodes.add(neighbor)

        slice_node_ids.update(additional_nodes)
        logging.info(f"âš™ï¸ [{mode}] åˆ‡å‰²å¾Œç¯€é»æ•¸é‡: {len(slice_node_ids)}")
        return _build_sliced_data_optimized(data, slice_node_ids)

    except Exception as e:
        logging.error(f"âŒ [{mode}] åˆ‡å‰²å¤±æ•—: {e}")
        return data

def process_pt_dataset_memory_safe(pt_path, output_path, batch_size=10):
    """è¨˜æ†¶é«”å®‰å…¨ç‰ˆæœ¬ï¼šè¶…å°æ‰¹æ¬¡ + å³æ™‚ä¿å­˜"""
    data = torch.load(pt_path)
    
    total_items = len(data)
    
    # ä½¿ç”¨è‡¨æ™‚æª”æ¡ˆåˆ†æ®µä¿å­˜
    temp_dir = "/home/tu/exp/EXP_final/preprocessing/graph_processing"
    os.makedirs(temp_dir, exist_ok=True)
    
    processed_count = 0
    total_times = 0
    with tqdm(total=total_items, desc="Memory-Safe Processing") as pbar:
        start_time = time.time()
        for batch_start in range(0, total_items, batch_size):
            batch_end = min(batch_start + batch_size, total_items)
            batch_items = data[batch_start:batch_end]
            
            batch_processed = []
            
            for idx, item in enumerate(batch_items):
                # if item['cwe_id'] != 'normal' and item['cwe_id'] != '':
                #     continue
                actual_idx = batch_start + idx
                
                try:
                    # è™•ç†å–®ä¸€é …ç›®

                    processed_item = process_single_item(item, actual_idx)
                    if processed_item:
                        batch_processed.append(processed_item)
                        processed_count += 1
                    
                    # æ›´æ–°é€²åº¦
                    pbar.update(1)
                    pbar.set_postfix({
                        'å·²è™•ç†': processed_count,
                        'è¨˜æ†¶é«”': f"{psutil.Process().memory_info().rss / 1024 / 1024:.1f}MB"
                    })
                    
                except Exception as e:
                    logging.error(f"âŒ ç¬¬ {actual_idx} ç­†è™•ç†å¤±æ•—: {e}")
                    pbar.update(1)
                    continue
            end_time = time.time()

            # # è¨ˆç®—ä¸¦è¨˜éŒ„è™•ç†æ™‚é–“
            processing_time = end_time - start_time
            total_times += processing_time
            # logging.info(f"âœ… ç¬¬ {actual_idx} ç­†è™•ç†å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’")

            # ä¿å­˜æ‰¹æ¬¡çµæœåˆ°è‡¨æ™‚æª”æ¡ˆ
            temp_file = os.path.join(temp_dir, f"batch_{batch_start}.pt")
            torch.save(batch_processed, temp_file)
            
            # å¼·åˆ¶æ¸…ç†è¨˜æ†¶é«”
            del batch_processed, batch_items
            gc.collect()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    # è¨ˆç®—å¹³å‡è£åˆ‡æ™‚é–“
    average_slice_time = total_times / max(processed_count, 1)
    logging.info(f"ğŸ“Š å¹³å‡è£åˆ‡æ™‚é–“ï¼š{average_slice_time:.4f} ç§’")
    # åˆä½µæ‰€æœ‰è‡¨æ™‚æª”æ¡ˆ
    merge_temp_files(temp_dir, output_path)
    
    # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
    import shutil
    shutil.rmtree(temp_dir)
    
    logging.info(f"âœ… è¨˜æ†¶é«”å®‰å…¨è™•ç†å®Œæˆï¼è™•ç†äº† {processed_count} ç­†è³‡æ–™ï¼Œæ™‚é–“: {total_times:.2f}ç§’")

def process_single_item(item, idx):
    """è™•ç†å–®ä¸€é …ç›®ï¼Œç«‹å³é‡‹æ”¾è¨˜æ†¶é«”"""
    if 'full_graph' not in item or item['full_graph'] is None:
        return None
    
    graph_data = item['full_graph']
    is_vul = 'cwe_id' in item and item['cwe_id'] is not None and item['cwe_id'] != '' and item['cwe_id'] != 'normal'
    
    try:
        # è™•ç†é‚è¼¯
        if is_vul:
            target_lines = item['buggy_lines'].keys()
        
        if target_lines:
            sliced_data_buggy = slice_graph(graph_data, mode=SLICE_MODE, target_lines=target_lines)
            item['sliced_graph_buggy'] = sliced_data_buggy
        else:
            logging.warning(f"âš ï¸ ç¬¬ {idx} ç­†è³‡æ–™ç„¡ target_lines")

        # ğŸ” å¯é¸ï¼šä¹Ÿè£ CALL/IDENTIFIERï¼ˆV2 æ¨¡å¼ï¼‰
        # sliced_data_node_type = slice_graph(graph_data, mode=SLICE_MODE)
        # item['sliced_graph_node_type'] = sliced_data_node_type

        
        # ç«‹å³æ¸…ç†åŸåœ–ä»¥ç¯€çœè¨˜æ†¶é«”
        # del item['full_graph']
        
        return item
        
    except Exception as e:
        logging.error(f"è™•ç†é …ç›® {idx} å¤±æ•—: {e}")
        return None

def merge_temp_files(temp_dir, output_path):
    """åˆä½µè‡¨æ™‚æª”æ¡ˆ"""
    all_data = []
    
    temp_files = sorted([f for f in os.listdir(temp_dir) if f.endswith('.pt')])
    
    for temp_file in tqdm(temp_files, desc="åˆä½µæª”æ¡ˆ"):
        temp_path = os.path.join(temp_dir, temp_file)
        batch_data = torch.load(temp_path)
        all_data.extend(batch_data)
        
        # ç«‹å³åˆªé™¤å·²è®€å–çš„è‡¨æ™‚æª”æ¡ˆ
        os.remove(temp_path)
    
    # ä¿å­˜æœ€çµ‚çµæœ
    torch.save(all_data, output_path)
    logging.info(f"âœ… åˆä½µå®Œæˆï¼Œç¸½å…± {len(all_data)} ç­†è³‡æ–™")



def main():
    for pt, out in [(EXAMPLE_PT, EXAMPLE_OUT_PT)]:
    # for pt, out in [(TRAIN_PT, TRAIN_OUT), (EVAL_PT, EVAL_OUT), (TEST_PT, TEST_OUT)]:
        if os.path.exists(pt):
            process_pt_dataset_memory_safe(pt, out, batch_size=50)
        else:
            logging.warning(f"âŒ File not found: {pt}")

if __name__ == "__main__":
    main()