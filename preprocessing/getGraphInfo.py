import json
import threading
import concurrent.futures
from concurrent.futures import ProcessPoolExecutor
import time
import torch
from collections import defaultdict
from tqdm import tqdm
import logging
import os
import subprocess
import pandas as pd


# === è·¯å¾‘è¨­å®š ===
BASE_DIR = "/home/tu/exp/EXP_final/Example_output"
JSON_PATH = os.path.join(BASE_DIR, "result.jsonl")
INPUT_DIR = os.path.join(BASE_DIR, "outputBin")
OUTPUT_DIR = os.path.join(BASE_DIR, "graph")
LOG_PATH = os.path.join(BASE_DIR, "output_get_graph_info.log")
JOERN_PATH = "joern"
SCRIPT_PATH = "/home/tu/exp/EXP_final/joern/funcToGraph.sc"


# === Logging è¨­å®š ===
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler(LOG_PATH, mode="a"),
                              logging.StreamHandler()])

# === è¼”åŠ©å‡½å¼ ===
def filter_processed_files(bin_files, output_dir):
    unprocessed, already_processed = [], []
    for bin_file in bin_files:
        output_file = bin_file.replace('.java.bin', '.json') if bin_file.endswith('.java.bin') else bin_file.replace('.bin', '.json')
        output_path = os.path.join(output_dir, output_file)
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            already_processed.append(bin_file)
        else:
            unprocessed.append(bin_file)
    return unprocessed, already_processed


def joern_create_graph_single(joern_path, input_bin_path, output_dir, bin_file, script_path):
    """
    è‡ªå‹•åŒ–è™•ç†å·²ç¶“ joern-parse éçš„ bin æ–‡ä»¶
    
    Args:
        joern_path: joern å¯åŸ·è¡Œæ–‡ä»¶è·¯å¾‘
        input_bin_path: åŒ…å« .bin æ–‡ä»¶çš„ç›®éŒ„
        output_dir: è¼¸å‡º .pt æ–‡ä»¶çš„ç›®éŒ„  
        bin_files: è¦è™•ç†çš„ bin æ–‡ä»¶åˆ—è¡¨
        script_path: funcToGraph.sc çš„è·¯å¾‘
    """
    
    # å•Ÿå‹• joern é€²ç¨‹
    joern_process = subprocess.Popen(
        [joern_path], 
        stdin=subprocess.PIPE, 
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        bufsize=0
    )
    
    processed_files = []
    
    try:
        # for bin_file in tqdm(bin_files, desc="Processing bin files"):
            
        # 1. æ§‹å»ºå®Œæ•´è·¯å¾‘
        bin_full_path = os.path.abspath(os.path.join(input_bin_path, bin_file))
        
        # 2. æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(bin_full_path):
            logging.warning(f"Warning: {bin_full_path} not found, skipping...")
            return None
        
        # 3. ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶å (å»æ‰ .java.bin å¾Œç¶´ï¼ŒåŠ ä¸Š .pt)
        if bin_file.endswith('.java.bin'):
            output_name = bin_file.replace('.java.bin', '.json')
        elif bin_file.endswith('.bin'):
            output_name = bin_file.replace('.bin', '.json')
        else:
            output_name = bin_file + '.json'
        
        output_full_path = os.path.abspath(os.path.join(output_dir, output_name))

        logging.info(f"Input: {bin_full_path}")
        logging.info(f"Output: {output_full_path}")
        
        # 4. åŸ·è¡Œ joern å‘½ä»¤åºåˆ—
        try:
            logging.info(f'\n=== BEGIN processing {bin_file} ===')
            
            # Step 1: importCpg
            import_cmd = f"importCpg(\"{bin_full_path}\")\n"
            logging.info(f"Executing: {import_cmd.strip()}")
            joern_process.stdin.write(import_cmd)
            joern_process.stdin.flush()
            time.sleep(1)

            
            # Step 2: è¨­ç½® outputPath è®Šæ•¸
            output_var_cmd = f'val outputPath = "{output_full_path}"\n'
            logging.info(f"Executing: {output_var_cmd.strip()}")
            joern_process.stdin.write(output_var_cmd)
            joern_process.stdin.flush()
            time.sleep(0.5)
            
            # Step 3: è¼‰å…¥ä¸¦åŸ·è¡Œè…³æœ¬
            script_cmd = f':load {script_path}\n'
            logging.info(f"Executing: {script_cmd.strip()}")
            joern_process.stdin.write(script_cmd)
            joern_process.stdin.flush()
            
            
            # ç­‰å¾…è…³æœ¬åŸ·è¡Œå®Œæˆ
            time.sleep(5)
            
            # Step 4: æ¸…ç†ç•¶å‰ CPG
            delete_cmd = 'delete\n'
            logging.info(f"Executing: {delete_cmd.strip()}")
            joern_process.stdin.write(delete_cmd)
            joern_process.stdin.flush()
            
            # ç­‰å¾…æ¸…ç†å®Œæˆ
            time.sleep(1)
            
            processed_files.append(output_name)
            logging.info(f"âœ… Successfully processed {bin_file} -> {output_name}")
        
        except Exception as e:
            logging.error(f"âŒ Error processing {bin_file}: {str(e)}")
            return None

    finally:
        logging.info("=== END processing ===")     
        # é—œé–‰ joern é€²ç¨‹
        try:
            joern_process.stdin.write('exit\n')
            joern_process.stdin.flush()
            joern_process.wait(timeout=5)
        except subprocess.TimeoutExpired:
            joern_process.kill()
            joern_process.wait()
    
    return processed_files
        
def batch_process_bins(json_path, joern_path, input_dir, output_dir, script_path, skip_processed=True):
    """
    æ‰¹æ¬¡è™•ç†ç›®éŒ„ä¸­çš„æ‰€æœ‰ bin æ–‡ä»¶
    
    Args:
        joern_path: joern å®‰è£è·¯å¾‘ (ä¾‹å¦‚: "joern-cli/joern")
        input_dir: åŒ…å« .bin æ–‡ä»¶çš„ç›®éŒ„
        output_dir: è¼¸å‡º .pt æ–‡ä»¶çš„ç›®éŒ„
        script_path: funcToGraph_fixed.sc çš„å®Œæ•´è·¯å¾‘
    """
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è¼‰ result.jsonl æ–‡ä»¶
    filtered_bin_files = []
    try:
        # è®€å– JSON æ–‡ä»¶
        # df = pd.read_json(json_path)
        # df['bin_file_name'] = df.apply(
        #     lambda row: f"{row['file_path']}.java.bin", axis=1
        # )
        # è®€å– JSONL æ–‡ä»¶
        df = pd.read_json(json_path, lines=True)
        df_filtered = df[df['cwe_id']!=""]  # éæ¿¾ cwe æ¬„ä½ä¸æ˜¯ç©ºçš„è¨˜éŒ„
        df_filtered['bin_file_name'] = df_filtered.apply(
            lambda row: f"{row['apk_name']}_{row['file'].replace('/', '_')}.bin", axis=1
        )
        filtered_bin_files = df_filtered['bin_file_name'].tolist()
    
    except Exception as e:
        logging.error(f"âŒ Error reading result.jsonl file: {str(e)}")
        return []
    
    
    # æ‰¾åˆ°æ‰€æœ‰ bin æ–‡ä»¶
    all_bin_files = [f for f in os.listdir(input_dir) if f.endswith('.bin')]
    
    if not all_bin_files:
        logging.warning(f"No .bin files found in {input_dir}")
        return []
    
    logging.info(f"Found {len(all_bin_files)} total bin files")
    
    
    # éæ¿¾ bin æ–‡ä»¶ï¼Œåƒ…ä¿ç•™åœ¨ filtered_bin_files ä¸­çš„æ–‡ä»¶
    bin_files_to_process = [f for f in all_bin_files if f in filtered_bin_files]
    if not bin_files_to_process:
        logging.info("ğŸ‰ No matching bin files to process based on result.jsonl!")
        return []
    
    logging.info(f"Filtered {len(bin_files_to_process)} bin files to process based on result.jsonl")
    
    
    # éæ¿¾å·²è™•ç†çš„æ–‡ä»¶
    if skip_processed:
        bin_files, already_processed = filter_processed_files(bin_files_to_process, output_dir)
        if not bin_files:
            logging.info("ğŸ‰ All files already processed! Nothing to do.")
            return already_processed
    else:
        bin_files = bin_files_to_process
        already_processed = []
    
    newly_processed = []

    for bin_file in tqdm(bin_files, desc="Processing bin files"):
        logging.info(f"Processing file: {bin_file}")
        try:
            # è™•ç†å‰©é¤˜çš„æ–‡ä»¶
            newly_processed_single = joern_create_graph_single(
                joern_path=joern_path,
                input_bin_path=input_dir,
                output_dir=output_dir,
                bin_file=bin_file,
                script_path=script_path
            )
            
            if newly_processed_single:
                newly_processed.extend(newly_processed_single)
        except Exception as e:
            logging.error(f"âŒ Error processing {bin_file}: {str(e)}")
            continue                
            
    all_processed = already_processed + newly_processed

    logging.info(f"\nğŸ‰ Processing complete!")
    logging.info(f"ğŸ“Š Summary:")
    logging.info(f"  Previously processed: {len(already_processed)}")
    logging.info(f"  Newly processed: {len(newly_processed)}")
    logging.info(f"  Total processed: {len(all_processed)}")
    
    return all_processed


if __name__ == "__main__":
    result = batch_process_bins(
        json_path=JSON_PATH,
        joern_path=JOERN_PATH,
        input_dir=INPUT_DIR,
        output_dir=OUTPUT_DIR,
        script_path=SCRIPT_PATH,
        skip_processed=True
    )
    logging.info(f"Processed {len(result)} files.")
